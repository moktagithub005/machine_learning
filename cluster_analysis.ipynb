{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b74a76d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GaussianMixture' from 'sklearn.cluster' (C:\\Users\\ajaym\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (KMeans, AgglomerativeClustering, DBSCAN, \n\u001b[0;32m      6\u001b[0m                            SpectralClustering, GaussianMixture, MeanShift, \n\u001b[0;32m      7\u001b[0m                            AffinityPropagation, OPTICS, Birch)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'GaussianMixture' from 'sklearn.cluster' (C:\\Users\\ajaym\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import (KMeans, AgglomerativeClustering, DBSCAN, \n",
    "                           SpectralClustering, GaussianMixture, MeanShift, \n",
    "                           AffinityPropagation, OPTICS, Birch)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (silhouette_score, adjusted_rand_score, \n",
    "                           normalized_mutual_info_score, calinski_harabasz_score,\n",
    "                           davies_bouldin_score)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Creating the DataFrame\n",
    "data = {\n",
    "    'Car Model': ['Toyota Aygo', 'Mitsubishi Space Star', 'Skoda Citigo', 'Fiat 500', 'Mini Cooper', 'VW Up!', 'Skoda Fabia', \n",
    "                  'Mercedes A-Class', 'Ford Fiesta', 'Audi A1', 'Hyundai I20', 'Suzuki Swift', 'Ford Fiesta', 'Honda Civic', \n",
    "                  'Hyundai I30', 'Opel Astra', 'BMW 1', 'Mazda 3', 'Skoda Rapid', 'Ford Focus', 'Ford Mondeo', 'Opel Insignia', \n",
    "                  'Mercedes C-Class', 'Skoda Octavia', 'Volvo S60', 'Mercedes CLA', 'Audi A4', 'Audi A6', 'Volvo V70', \n",
    "                  'BMW 5', 'Mercedes E-Class', 'Volvo XC70', 'Ford B-Max', 'BMW 2', 'Opel Zafira', 'Mercedes SLK'],\n",
    "    'Volume': [1000, 1200, 1000, 900, 1500, 1000, 1400, 1500, 1500, 1600, 1100, 1300, 1000, 1600, 1600, 1600, 1600, 2200, 1600, 2000,\n",
    "               1600, 2000, 2100, 1600, 2000, 1500, 2000, 2000, 1600, 2000, 2100, 1600, 1600, 1600, 2500, 1800],\n",
    "    'Weight': [790, 1160, 929, 865, 1140, 929, 1109, 1365, 1112, 1150, 980, 990, 1112, 1252, 1326, 1330, 1365, 1280, 1119, 1328,\n",
    "               1584, 1428, 1365, 1415, 1415, 1465, 1490, 1725, 1523, 1705, 1605, 1746, 1235, 1390, 1405, 1395],\n",
    "    'CO2': [99, 95, 95, 90, 105, 105, 90, 92, 98, 99, 99, 101, 99, 94, 97, 97, 99, 104, 104, 105,\n",
    "            94, 99, 99, 99, 99, 102, 104, 114, 109, 114, 115, 117, 104, 108, 109, 120]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE CLUSTERING ANALYSIS FOR CAR DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features for clustering: Volume, Weight, CO2\")\n",
    "print(\"\\nDataset summary:\")\n",
    "print(df[['Volume', 'Weight', 'CO2']].describe())\n",
    "\n",
    "# Prepare data for clustering\n",
    "features = ['Volume', 'Weight', 'CO2']\n",
    "X = df[features].values\n",
    "\n",
    "# Data scaling\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "X_standard = scaler_standard.fit_transform(X)\n",
    "X_minmax = scaler_minmax.fit_transform(X)\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "print(\"Data preprocessing completed (StandardScaler and MinMaxScaler)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. OPTIMAL NUMBER OF CLUSTERS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. OPTIMAL NUMBER OF CLUSTERS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def plot_elbow_and_silhouette():\n",
    "    \"\"\"Plot elbow method and silhouette analysis\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Elbow Method for K-Means\n",
    "    K_range = range(2, 11)\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_standard)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_standard, kmeans.labels_))\n",
    "    \n",
    "    # Elbow curve\n",
    "    axes[0, 0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 0].set_ylabel('Inertia (Within-cluster sum of squares)')\n",
    "    axes[0, 0].set_title('Elbow Method for Optimal k')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Silhouette scores\n",
    "    axes[0, 1].plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 1].set_ylabel('Silhouette Score')\n",
    "    axes[0, 1].set_title('Silhouette Analysis')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calinski-Harabasz Index\n",
    "    ch_scores = []\n",
    "    db_scores = []\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_standard)\n",
    "        ch_scores.append(calinski_harabasz_score(X_standard, labels))\n",
    "        db_scores.append(davies_bouldin_score(X_standard, labels))\n",
    "    \n",
    "    axes[1, 0].plot(K_range, ch_scores, 'go-', linewidth=2, markersize=8)\n",
    "    axes[1, 0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[1, 0].set_ylabel('Calinski-Harabasz Index')\n",
    "    axes[1, 0].set_title('Calinski-Harabasz Index (Higher is Better)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Davies-Bouldin Index\n",
    "    axes[1, 1].plot(K_range, db_scores, 'mo-', linewidth=2, markersize=8)\n",
    "    axes[1, 1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[1, 1].set_ylabel('Davies-Bouldin Index')\n",
    "    axes[1, 1].set_title('Davies-Bouldin Index (Lower is Better)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal k\n",
    "    optimal_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "    optimal_k_ch = K_range[np.argmax(ch_scores)]\n",
    "    optimal_k_db = K_range[np.argmin(db_scores)]\n",
    "    \n",
    "    print(f\"Optimal k based on Silhouette Score: {optimal_k_silhouette}\")\n",
    "    print(f\"Optimal k based on Calinski-Harabasz Index: {optimal_k_ch}\")\n",
    "    print(f\"Optimal k based on Davies-Bouldin Index: {optimal_k_db}\")\n",
    "    \n",
    "    return optimal_k_silhouette\n",
    "\n",
    "optimal_k = plot_elbow_and_silhouette()\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CLUSTERING ALGORITHMS IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"2. IMPLEMENTING MULTIPLE CLUSTERING ALGORITHMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "# Define clustering algorithms\n",
    "clustering_algorithms = {\n",
    "    'K-Means': KMeans(n_clusters=optimal_k, random_state=42, n_init=10),\n",
    "    'K-Means (k=3)': KMeans(n_clusters=3, random_state=42, n_init=10),\n",
    "    'K-Means (k=4)': KMeans(n_clusters=4, random_state=42, n_init=10),\n",
    "    'Agglomerative (Ward)': AgglomerativeClustering(n_clusters=optimal_k, linkage='ward'),\n",
    "    'Agglomerative (Complete)': AgglomerativeClustering(n_clusters=optimal_k, linkage='complete'),\n",
    "    'Agglomerative (Average)': AgglomerativeClustering(n_clusters=optimal_k, linkage='average'),\n",
    "    'DBSCAN (eps=0.5)': DBSCAN(eps=0.5, min_samples=3),\n",
    "    'DBSCAN (eps=0.8)': DBSCAN(eps=0.8, min_samples=3),\n",
    "    'DBSCAN (eps=1.0)': DBSCAN(eps=1.0, min_samples=3),\n",
    "    'Spectral Clustering': SpectralClustering(n_clusters=optimal_k, random_state=42),\n",
    "    'Gaussian Mixture': GaussianMixture(n_components=optimal_k, random_state=42),\n",
    "    'Mean Shift': MeanShift(),\n",
    "    'Affinity Propagation': AffinityPropagation(random_state=42),\n",
    "    'OPTICS': OPTICS(min_samples=3),\n",
    "    'Birch': Birch(n_clusters=optimal_k)\n",
    "}\n",
    "\n",
    "print(\"Applying clustering algorithms...\")\n",
    "\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    try:\n",
    "        if 'DBSCAN' in name or name in ['Mean Shift', 'Affinity Propagation', 'OPTICS']:\n",
    "            labels = algorithm.fit_predict(X_standard)\n",
    "        else:\n",
    "            labels = algorithm.fit_predict(X_standard)\n",
    "        \n",
    "        # Calculate metrics (skip if only one cluster or noise points)\n",
    "        unique_labels = np.unique(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "        \n",
    "        if n_clusters > 1 and len(unique_labels) > 1:\n",
    "            silhouette = silhouette_score(X_standard, labels)\n",
    "            if n_clusters > 1:\n",
    "                ch_score = calinski_harabacz_score(X_standard, labels)\n",
    "                db_score = davies_bouldin_score(X_standard, labels)\n",
    "            else:\n",
    "                ch_score = 0\n",
    "                db_score = float('inf')\n",
    "        else:\n",
    "            silhouette = -1\n",
    "            ch_score = 0\n",
    "            db_score = float('inf')\n",
    "        \n",
    "        clustering_results[name] = {\n",
    "            'labels': labels,\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette_score': silhouette,\n",
    "            'calinski_harabasz_score': ch_score,\n",
    "            'davies_bouldin_score': db_score,\n",
    "            'algorithm': algorithm\n",
    "        }\n",
    "        \n",
    "        print(f\"{name:25s} | Clusters: {n_clusters:2d} | Silhouette: {silhouette:6.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {str(e)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. CLUSTERING RESULTS COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"3. CLUSTERING RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, results in clustering_results.items():\n",
    "    comparison_data.append({\n",
    "        'Algorithm': name,\n",
    "        'Clusters': results['n_clusters'],\n",
    "        'Silhouette': results['silhouette_score'],\n",
    "        'Calinski-Harabasz': results['calinski_harabasz_score'],\n",
    "        'Davies-Bouldin': results['davies_bouldin_score']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Silhouette', ascending=False)\n",
    "\n",
    "print(\"Clustering Algorithm Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Algorithm':<25} {'Clusters':<8} {'Silhouette':<12} {'C-H Index':<12} {'D-B Index':<10}\")\n",
    "print(\"-\"*70)\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['Algorithm']:<25} {row['Clusters']:<8} {row['Silhouette']:<12.3f} {row['Calinski-Harabasz']:<12.1f} {row['Davies-Bouldin']:<10.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. VISUALIZATION OF CLUSTERING RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"4. CLUSTERING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top performing algorithms for visualization\n",
    "top_algorithms = comparison_df.head(8)['Algorithm'].tolist()\n",
    "\n",
    "def plot_clustering_results():\n",
    "    \"\"\"Create comprehensive clustering visualizations\"\"\"\n",
    "    \n",
    "    # 1. 3D Scatter plots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    for i, algo_name in enumerate(top_algorithms):\n",
    "        ax = fig.add_subplot(2, 4, i+1, projection='3d')\n",
    "        labels = clustering_results[algo_name]['labels']\n",
    "        \n",
    "        # Create color map\n",
    "        unique_labels = np.unique(labels)\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for label, color in zip(unique_labels, colors):\n",
    "            if label == -1:  # Noise points\n",
    "                mask = labels == label\n",
    "                ax.scatter(X[mask, 0], X[mask, 1], X[mask, 2], \n",
    "                          c='black', marker='x', s=50, alpha=0.6, label='Noise')\n",
    "            else:\n",
    "                mask = labels == label\n",
    "                ax.scatter(X[mask, 0], X[mask, 1], X[mask, 2], \n",
    "                          c=[color], s=50, alpha=0.7, label=f'Cluster {label}')\n",
    "        \n",
    "        ax.set_xlabel('Volume')\n",
    "        ax.set_ylabel('Weight')\n",
    "        ax.set_zlabel('CO2')\n",
    "        ax.set_title(f'{algo_name}\\n({clustering_results[algo_name][\"n_clusters\"]} clusters)')\n",
    "        \n",
    "        if len(unique_labels) <= 6:  # Only show legend if not too many clusters\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. 2D Projections\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    feature_pairs = [('Volume', 'Weight'), ('Volume', 'CO2'), ('Weight', 'CO2')]\n",
    "    \n",
    "    for i, algo_name in enumerate(top_algorithms[:9]):\n",
    "        ax = axes[i]\n",
    "        labels = clustering_results[algo_name]['labels']\n",
    "        \n",
    "        # Plot Volume vs Weight\n",
    "        unique_labels = np.unique(labels)\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for label, color in zip(unique_labels, colors):\n",
    "            if label == -1:\n",
    "                mask = labels == label\n",
    "                ax.scatter(df.loc[mask, 'Volume'], df.loc[mask, 'Weight'], \n",
    "                          c='black', marker='x', s=30, alpha=0.6)\n",
    "            else:\n",
    "                mask = labels == label\n",
    "                ax.scatter(df.loc[mask, 'Volume'], df.loc[mask, 'Weight'], \n",
    "                          c=[color], s=30, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Volume')\n",
    "        ax.set_ylabel('Weight')\n",
    "        ax.set_title(f'{algo_name}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_clustering_results()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. HIERARCHICAL CLUSTERING DENDROGRAM\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"5. HIERARCHICAL CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def plot_dendrograms():\n",
    "    \"\"\"Plot dendrograms for different linkage methods\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "    \n",
    "    for i, method in enumerate(linkage_methods):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        # Calculate linkage\n",
    "        if method == 'ward':\n",
    "            linkage_matrix = linkage(X_standard, method=method)\n",
    "        else:\n",
    "            linkage_matrix = linkage(X_standard, method=method, metric='euclidean')\n",
    "        \n",
    "        # Plot dendrogram\n",
    "        dendrogram(linkage_matrix, ax=ax, truncate_mode='level', p=5)\n",
    "        ax.set_title(f'Dendrogram - {method.capitalize()} Linkage')\n",
    "        ax.set_xlabel('Sample Index or (Cluster Size)')\n",
    "        ax.set_ylabel('Distance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_dendrograms()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. PCA VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"6. PCA-BASED CLUSTERING VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standard)\n",
    "\n",
    "print(f\"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total Variance Explained: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "def plot_pca_clusters():\n",
    "    \"\"\"Plot clustering results in PCA space\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, algo_name in enumerate(top_algorithms):\n",
    "        ax = axes[i]\n",
    "        labels = clustering_results[algo_name]['labels']\n",
    "        \n",
    "        unique_labels = np.unique(labels)\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for label, color in zip(unique_labels, colors):\n",
    "            if label == -1:\n",
    "                mask = labels == label\n",
    "                ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                          c='black', marker='x', s=30, alpha=0.6, label='Noise')\n",
    "            else:\n",
    "                mask = labels == label\n",
    "                ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                          c=[color], s=30, alpha=0.7, label=f'C{label}')\n",
    "        \n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "        ax.set_title(f'{algo_name}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if len(unique_labels) <= 5:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_pca_clusters()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. DETAILED ANALYSIS OF BEST CLUSTERING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"7. DETAILED ANALYSIS OF BEST CLUSTERING ALGORITHM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select best algorithm based on silhouette score\n",
    "best_algo_name = comparison_df.iloc[0]['Algorithm']\n",
    "best_results = clustering_results[best_algo_name]\n",
    "best_labels = best_results['labels']\n",
    "\n",
    "print(f\"Best Algorithm: {best_algo_name}\")\n",
    "print(f\"Number of Clusters: {best_results['n_clusters']}\")\n",
    "print(f\"Silhouette Score: {best_results['silhouette_score']:.3f}\")\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_clustered = df.copy()\n",
    "df_clustered['Cluster'] = best_labels\n",
    "\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "cluster_counts = pd.Series(best_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    if cluster == -1:\n",
    "        print(f\"Noise Points: {count}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster}: {count} cars\")\n",
    "\n",
    "print(f\"\\nCluster Statistics:\")\n",
    "print(\"=\"*50)\n",
    "for cluster in sorted(np.unique(best_labels)):\n",
    "    if cluster != -1:\n",
    "        cluster_data = df_clustered[df_clustered['Cluster'] == cluster]\n",
    "        print(f\"\\nCluster {cluster} ({len(cluster_data)} cars):\")\n",
    "        print(f\"  Volume: {cluster_data['Volume'].mean():.1f} ± {cluster_data['Volume'].std():.1f}\")\n",
    "        print(f\"  Weight: {cluster_data['Weight'].mean():.1f} ± {cluster_data['Weight'].std():.1f}\")\n",
    "        print(f\"  CO2: {cluster_data['CO2'].mean():.1f} ± {cluster_data['CO2'].std():.1f}\")\n",
    "        print(f\"  Cars: {', '.join(cluster_data['Car Model'].tolist()[:5])}\" + \n",
    "              ('...' if len(cluster_data) > 5 else ''))\n",
    "\n",
    "# =============================================================================\n",
    "# 8. CLUSTER PROFILING AND INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"8. CLUSTER PROFILING AND BUSINESS INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_clusters():\n",
    "    \"\"\"Provide business insights from clustering\"\"\"\n",
    "    \n",
    "    # Create cluster profiles\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Box plots for each feature by cluster\n",
    "    features_to_plot = ['Volume', 'Weight', 'CO2']\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        if i < 3:\n",
    "            ax = axes[i//2, i%2]\n",
    "            df_clustered.boxplot(column=feature, by='Cluster', ax=ax)\n",
    "            ax.set_title(f'{feature} Distribution by Cluster')\n",
    "            ax.set_xlabel('Cluster')\n",
    "            ax.set_ylabel(feature)\n",
    "    \n",
    "    # Correlation heatmap for the best clustering\n",
    "    ax = axes[1, 1]\n",
    "    cluster_means = df_clustered.groupby('Cluster')[['Volume', 'Weight', 'CO2']].mean()\n",
    "    sns.heatmap(cluster_means.T, annot=True, cmap='viridis', ax=ax)\n",
    "    ax.set_title('Cluster Centroids Heatmap')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Business insights\n",
    "    print(\"BUSINESS INSIGHTS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for cluster in sorted(np.unique(best_labels)):\n",
    "        if cluster != -1:\n",
    "            cluster_data = df_clustered[df_clustered['Cluster'] == cluster]\n",
    "            avg_volume = cluster_data['Volume'].mean()\n",
    "            avg_weight = cluster_data['Weight'].mean()\n",
    "            avg_co2 = cluster_data['CO2'].mean()\n",
    "            \n",
    "            print(f\"\\nCluster {cluster} Profile:\")\n",
    "            if avg_volume < 1300 and avg_weight < 1100:\n",
    "                print(\"  → Compact/Economy Cars: Small engine, lightweight, eco-friendly\")\n",
    "            elif avg_volume > 1800 and avg_weight > 1500:\n",
    "                print(\"  → Large/Luxury Cars: High-performance, heavier, higher emissions\")\n",
    "            else:\n",
    "                print(\"  → Mid-size Cars: Balanced performance and efficiency\")\n",
    "            \n",
    "            print(f\"  → Target Market: {'Economy-conscious' if avg_co2 < 100 else 'Performance-oriented' if avg_co2 > 110 else 'Mainstream'}\")\n",
    "            print(f\"  → Environmental Impact: {'Low' if avg_co2 < 100 else 'High' if avg_co2 > 110 else 'Moderate'}\")\n",
    "\n",
    "analyze_clusters()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERING ANALYSIS COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total algorithms tested: {len(clustering_algorithms)}\")\n",
    "print(f\"Best performing algorithm: {best_algo_name}\")\n",
    "print(f\"Optimal number of clusters: {best_results['n_clusters']}\")\n",
    "print(f\"Best silhouette score: {best_results['silhouette_score']:.3f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761ffac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080966cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6b7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a0219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
